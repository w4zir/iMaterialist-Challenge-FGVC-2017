{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iMaterialist Challenge at FGVC 2017\n",
    "\n",
    "Assign accurate description labels to images of apparel products.\n",
    "\n",
    "Links:\n",
    "    [iMaterialist Challenge at FGVC 2017](https://www.kaggle.com/c/imaterialist-challenge-FGVC2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read task and labels from json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/khan/workspace/ml_ws/datasets/imat_dataset/'\n",
    "\n",
    "def read_data():\n",
    "    task_json = os.path.join(data_dir,'fgvc4_iMat.task_map.json')\n",
    "    label_json = os.path.join(data_dir,'fgvc4_iMat.label_map.json')\n",
    "    train_json = os.path.join(data_dir,'fgvc4_iMat.train.data.json')\n",
    "    val_json = os.path.join(data_dir,'fgvc4_iMat.validation.data.json')\n",
    "    test_json = os.path.join(data_dir,'fgvc4_iMat.test.image.json')\n",
    "\n",
    "    with open(task_json) as task_json_file:    \n",
    "        task_json = json.load(task_json_file)\n",
    "\n",
    "    with open(label_json) as label_json_file:    \n",
    "        label_json = json.load(label_json_file)\n",
    "    \n",
    "    with open(train_json) as train_json_file:    \n",
    "        train_json = json.load(train_json_file)\n",
    "        \n",
    "    with open(val_json) as val_json_file:    \n",
    "        val_json = json.load(val_json_file)\n",
    "        \n",
    "    with open(test_json) as test_json_file:    \n",
    "        test_json = json.load(test_json_file)\n",
    "        \n",
    "    return task_json,label_json,train_json,val_json,test_json\n",
    "\n",
    "task_json,label_json,train_json,val_json,test_json = read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All task-label pair for an image per example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get task and label list for each image in train and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation has task and label pairs:\t 8432\n"
     ]
    }
   ],
   "source": [
    "# get task and label pairs for each image idx\n",
    "def get_task_labels(lst):\n",
    "    img_tasks_labels = {}\n",
    "    for row in lst:\n",
    "        if (row['imageId'] not in img_tasks_labels):       \n",
    "            task_label_set = {'dummy'}\n",
    "            for row2 in lst:\n",
    "                if (row2['imageId'] == row['imageId']):\n",
    "                    task_label_set.add(str(row2['taskId']+'_'+row2['labelId']))   \n",
    "            task_label_set.remove('dummy')\n",
    "        img_tasks_labels[row['imageId']] = task_label_set\n",
    "    return img_tasks_labels\n",
    "\n",
    "    \n",
    "# train_task_label = get_task_labels(train_json['annotations'])\n",
    "val_task_label = get_task_labels(val_json['annotations'])\n",
    "print(\"validation has task and label pairs:\\t\", len(val_task_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove bad ids from task and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad  validation  files:\t 299\n",
      "validation  task-pair before bad file removal:\t 8432\n",
      "value not found.\n",
      "validation  task-pair after bad file removal:\t 8134\n"
     ]
    }
   ],
   "source": [
    "def remove_bad_ids(bad_file, dict_task_label, data_type):\n",
    "    bad_file = open(bad_file, \"r\")\n",
    "    bad_ids = bad_file.read().split('\\n')\n",
    "    print('bad ', data_type,' files:\\t',len(bad_ids))\n",
    "    print(data_type,' task-pair before bad file removal:\\t',len(dict_task_label))\n",
    "\n",
    "    for x in bad_ids:\n",
    "        try:\n",
    "            del dict_task_label[x]\n",
    "        except:\n",
    "            print('value not found.')\n",
    "    print(data_type,' task-pair after bad file removal:\\t',len(dict_task_label))\n",
    "    \n",
    "val_task_label = remove_bad_ids(os.path.join(data_dir,'bad_val.txt'), val_task_label, 'validation')\n",
    "# train_task_label = remove_bad_ids(os.path.join(data_dir,'bad_train.txt'), train_task_label, 'train')\n",
    "# test_task_label = remove_bad_ids(os.path.join(data_dir,'bad_test.txt'), test_task_label, 'test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate output variable train_Y and val_Y containing one hot encoded labels and save them as numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_Y dimensions:\t (8134, 576)\n"
     ]
    }
   ],
   "source": [
    "# train_tl = train_task_label.values()\n",
    "# mlb_tr = MultiLabelBinarizer()\n",
    "# train_Y = mlb_tr.fit_transform(train_tl)\n",
    "\n",
    "val_tl = val_task_label.values()\n",
    "mlb_val = MultiLabelBinarizer()\n",
    "val_Y = mlb_val.fit_transform(val_tl)\n",
    "\n",
    "# print('train_Y dimensions:\\t', train_Y.shape)\n",
    "print('val_Y dimensions:\\t', val_Y.shape)\n",
    "\n",
    "# np.save('data/train_Y.npy', train_Y)\n",
    "np.save('data/val_Y.npy', val_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove bad ids i.e. images with no data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_dir = os.path.join(data_dir,'train_images/')\n",
    "# val_dir = os.path.join(data_dir,'val_images/')\n",
    "# test_dir = os.path.join(data_dir,'test_images/')\n",
    "\n",
    "# def remove_bad_ids():\n",
    "#     # Read images ids and remove bad data\\\n",
    "#     # train data\n",
    "#     train_ids = [f for f in os.listdir(train_dir) if f.endswith('.jpg')]\n",
    "#     train_ids = [os.path.splitext(f)[0] for f in train_ids]\n",
    "\n",
    "#     train_bad_file = os.path.join(data_dir,'bad_train.txt')\n",
    "#     train_bad_file = open(train_bad_file, \"r\")\n",
    "#     train_bad_ids = train_bad_file.read().split('\\n')\n",
    "\n",
    "#     train_ids = [x for x in train_ids if x not in train_bad_ids]\n",
    "    \n",
    "#     # validation data    \n",
    "#     val_ids = [f for f in os.listdir(val_dir) if f.endswith('.jpg')]\n",
    "#     val_ids = [os.path.splitext(f)[0] for f in val_ids]\n",
    "\n",
    "#     val_bad_file = os.path.join(data_dir,'bad_val.txt')\n",
    "#     val_bad_file = open(val_bad_file, \"r\")\n",
    "#     val_bad_ids = val_bad_file.read().split('\\n')\n",
    "\n",
    "#     val_ids = [x for x in val_ids if x not in val_bad_ids]\n",
    "    \n",
    "#     # test data\n",
    "#     test_ids = [f for f in os.listdir(test_dir) if f.endswith('.jpg')]\n",
    "#     test_ids = [os.path.splitext(f)[0] for f in test_ids]\n",
    "\n",
    "#     test_bad_file = os.path.join(data_dir,'bad_test.txt')\n",
    "#     test_bad_file = open(test_bad_file, \"r\")\n",
    "#     test_bad_ids = test_bad_file.read().split('\\n')\n",
    "\n",
    "#     test_ids = [x for x in test_ids if x not in test_bad_ids]\n",
    "    \n",
    "#     print('training data:\\t', len(train_ids))\n",
    "#     print('validation data:\\t', len(val_ids))\n",
    "#     print('test data:\\t', len(test_ids))\n",
    "#     return train_ids, val_ids, test_ids\n",
    "\n",
    "# train_ids, val_ids, test_ids = remove_bad_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_count = len(test_ids)\n",
    "# task_count = len(task_json['taskInfo'])\n",
    "# label_count = len(label_json['labelInfo'])\n",
    "\n",
    "# train_Y = np.zeros((len(train_json['annotations']),task_count*label_count))\n",
    "# val_Y = np.zeros((len(val_json['annotations']),task_count*label_count))\n",
    "\n",
    "# train_image_ids = []\n",
    "# idx = 0\n",
    "# for label in train_json['annotations']:\n",
    "#     task_id = int(label['taskId'])-1\n",
    "#     label_id = int(label['labelId'])-1\n",
    "#     train_Y[idx][task_id*label_id] = 1\n",
    "#     train_image_ids.append(label['imageId'])\n",
    "#     idx += 1\n",
    "    \n",
    "# val_image_ids = []\n",
    "# idx = 0\n",
    "# for label in val_json['annotations']:\n",
    "#     task_id = int(label['taskId'])-1\n",
    "#     label_id = int(label['labelId'])-1\n",
    "#     val_Y[idx][task_id*label_id] = 1\n",
    "#     val_image_ids.append(label['imageId'])\n",
    "#     idx += 1\n",
    "    \n",
    "# print('train_Y dimensions:\\t', train_Y.shape)\n",
    "# print('val_Y dimensions:\\t', val_Y.shape)\n",
    "\n",
    "# np.save('data/train_Y.npy', train_Y)\n",
    "# np.save('data/val_Y-.npy', val_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read images in train_X, val_X and test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE_PX = 100\n",
    "\n",
    "def load_images(img_dir, img_ids):\n",
    "    X = []\n",
    "    for i, image_name in enumerate(img_ids):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        img = cv2.imread(os.path.join(img_dir,image_name +'.jpg'))\n",
    "        img = cv2.resize(img, (IMG_SIZE_PX, IMG_SIZE_PX), interpolation = cv2.INTER_AREA)\n",
    "        X.append(img)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "validation data saved.\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(data_dir,'train_images/')\n",
    "val_dir = os.path.join(data_dir,'val_images/')\n",
    "test_dir = os.path.join(data_dir,'test_images/')\n",
    "IMG_SIZE_PX = 100\n",
    " \n",
    "# train_X = load_images(train_dir, train_ids)\n",
    "# np.save('data/train_X-{}-{}-{}.npy'.format(IMG_SIZE_PX,IMG_SIZE_PX,3), train_X)\n",
    "# print('training data saved.')\n",
    "\n",
    "val_X = load_images(val_dir, val_task_label.keys())\n",
    "np.save('data/val_X-{}-{}-{}.npy'.format(IMG_SIZE_PX,IMG_SIZE_PX,3), val_X)\n",
    "print('validation data saved.')\n",
    "\n",
    "# test_X = load_images(test_dir, test_ids)\n",
    "# np.save('data/test_X-{}-{}-{}.npy'.format(IMG_SIZE_PX,IMG_SIZE_PX,3), test_X)\n",
    "# print('testing data saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load saved images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_X = np.load('train_X-100-100-3.npy')\n",
    "val_X = np.load('data/val_X-100-100-3.npy')\n",
    "# test_X = np.load('test_X-100-100-3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('data/scene.png')\n",
    "resized_image = cv2.resize(img, (100, 100), interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One task-label pair per example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get task and label list for each image in train and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad train files:\t 1636\n",
      "train task-pair before bad file removal:\t 62088\n",
      "train after bad file removal:\t 59885\n"
     ]
    }
   ],
   "source": [
    "# get task and label pairs for each image idx and remove bad_ids\n",
    "def get_task_labels(lst, bad_file, data_type):\n",
    "    bad_file = open(bad_file, \"r\")\n",
    "    bad_ids = bad_file.read().split('\\n')\n",
    "    print('bad', data_type,'files:\\t',len(bad_ids))\n",
    "    print(data_type,'task-pair before bad file removal:\\t',len(lst))\n",
    "    \n",
    "    img_ids = []\n",
    "    img_tasks_labels = []\n",
    "    for row in lst:\n",
    "        if (row['imageId'] not in bad_ids):\n",
    "            img_ids.append(row['imageId'])\n",
    "            img_tasks_labels.append(row['taskId']+'_'+row['labelId'])\n",
    "    print(data_type, \"after bad file removal:\\t\", len(img_tasks_labels))\n",
    "    return img_ids, img_tasks_labels\n",
    "\n",
    "    \n",
    "train_ids, train_task_label = get_task_labels(train_json['annotations'], os.path.join(data_dir,'bad_train.txt'), 'train')\n",
    "# val_ids, val_task_label = get_task_labels(val_json['annotations'], os.path.join(data_dir,'bad_val.txt'), 'validation')\n",
    "# test_ids, test_task_label = get_task_labels(test_json['annotations'], os.path.join(data_dir,'bad_test.txt'), 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate output variable train_Y and val_Y containing one hot encoded labels and save them as numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Y dimensions:\t (59885, 576)\n"
     ]
    }
   ],
   "source": [
    "lb_tr = LabelBinarizer()\n",
    "train_Y = lb_tr.fit_transform(train_task_label)\n",
    "print('train_Y dimensions:\\t', train_Y.shape)\n",
    "np.save('data/train_Y.npy', train_Y)\n",
    "\n",
    "# lb_val = LabelBinarizer()\n",
    "# val_Y = lb_val.fit_transform(val_task_label)\n",
    "# print('val_Y dimensions:\\t', val_Y.shape)\n",
    "# np.save('data/val_Y.npy', val_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read images in train_X, val_X and test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE_PX = 100\n",
    "\n",
    "def load_images(img_dir, img_ids):\n",
    "    X = []\n",
    "    for i, image_name in enumerate(img_ids):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        img = cv2.imread(os.path.join(img_dir,image_name +'.jpg'))\n",
    "        img = cv2.resize(img, (IMG_SIZE_PX, IMG_SIZE_PX), interpolation = cv2.INTER_AREA)\n",
    "        X.append(img)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "training data saved.\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(data_dir,'train_images/')\n",
    "val_dir = os.path.join(data_dir,'val_images/')\n",
    "test_dir = os.path.join(data_dir,'test_images/')\n",
    "IMG_SIZE_PX = 100\n",
    " \n",
    "# train_X = load_images(train_dir, train_ids)\n",
    "# np.save('data/train_X-{}-{}-{}.npy'.format(IMG_SIZE_PX,IMG_SIZE_PX,3), train_X)\n",
    "# print('training data saved.')\n",
    "\n",
    "# val_X = load_images(val_dir, val_ids)\n",
    "# np.save('data/val_X-{}-{}-{}.npy'.format(IMG_SIZE_PX,IMG_SIZE_PX,3), val_X)\n",
    "# print('validation data saved.')\n",
    "\n",
    "# test_X = load_images(test_dir, test_ids)\n",
    "# np.save('data/test_X-{}-{}-{}.npy'.format(IMG_SIZE_PX,IMG_SIZE_PX,3), test_X)\n",
    "# print('testing data saved.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mllearn]",
   "language": "python",
   "name": "conda-env-mllearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
